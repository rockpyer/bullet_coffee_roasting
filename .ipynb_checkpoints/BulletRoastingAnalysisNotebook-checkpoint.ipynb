{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277f53d2-91e8-4a11-8ae9-b52523595329",
   "metadata": {},
   "source": [
    "### Data loading and analysis notebook for Allio Bullet R1 V2 roast data https://aillio.com/\n",
    "**by Ryan @f.w.Bennies https://www.instagram.com/f.w.bennies/**\n",
    "\n",
    "<img src=\"allRoastsPlt.png\" alt=\"friends\" width=\"270\"/> <img src=\"bulletRoastingEDA.png\" alt=\"friends\" width=\"210\"/><img src=\"friendshipsign.png\" alt=\"friends\" width=\"210\"/>\n",
    "\n",
    "\n",
    "#### Objectives\n",
    " Automaticaly load, serialize, and combine data from .json files. Then clean up unwanted data (non-standard batches)\n",
    " Split into curve and point data, create a few new features\n",
    " Summarize and display data (EDA) to enable data driven decisions in planning and real-time roasting\n",
    "\n",
    "#### Nice to haves:\n",
    " impute data (with confidence) rather than remove missing data\n",
    " generate additional new features\n",
    " incorporate bean density\n",
    "\n",
    "#### Excluding\n",
    " assocation for brewing techniques or results such as taste and aroma\n",
    " quantified color changes\n",
    "\n",
    "*Note: If your RoastTime isn't installed in the default MacOS location, edit 'base_path' (lines ~18-19)*\n",
    "\n",
    "*Built for my bullet (hardware version) purchased in July 2020 - noteworthy because the data structure of the .json files have changed over time which requires some merges or gap fixes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1842515-bf8d-421a-9703-8e1b3ce4d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "## open each .json in folder and append to df  ##\n",
    "#################################################\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "import scipy.stats as stats\n",
    "pd.set_option('display.max_columns', 55)\n",
    "plt.style.use('seaborn-colorblind')\n",
    "\n",
    "# Load from roasTime repository on macOS\n",
    "home = os.path.expanduser('~')\n",
    "base_path = os.path.join(home, 'Library/Application Support/roast-time/roasts')\n",
    "##  IF you have a specific set of roast profiles in another folder, uncomment the below\n",
    "# base_path = Path('/data')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for entry in os.listdir(base_path):\n",
    "    full_path =  Path('%s/%s' % (base_path, entry))\n",
    "    with full_path.open() as f:\n",
    "        data = json.loads(f.read())\n",
    "        df_load = pd.json_normalize(data)\n",
    "    df = pd.concat([df,df_load], ignore_index=True)\n",
    "\n",
    "# Export the raw DataFrame to a .csv file just for the record\n",
    "#create subfolder\n",
    "subfolder = 'csvExports/'\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "df.to_csv(subfolder + f\"raw_bullet-roasting_df.csv\", index=False)\n",
    "\n",
    "#############################\n",
    "##  First basic clean up   ##\n",
    "#############################\n",
    "\n",
    "# Sort and remove rows with missing data\n",
    "df.sort_values(by='dateTime', inplace=True)\n",
    "df.dropna(subset=['roastName'], inplace=True)\n",
    "\n",
    "# Filter dataframe to only your User ID\n",
    "\n",
    "choiceUserID = '73009f59-2d2e-4215-b6ff-961946ee0b80' ## enter specific userID (extracted from .json roast file, or RoastWorld web address)\n",
    "df = df.query('userId == @choiceUserID and isFork != 1.0')\n",
    "\n",
    "# Define list of unused data\n",
    "other_meta = ['userId', 'isFork', 'serialNumber', 'IRSensor', 'inventory.nextGreenWeight',\n",
    "              'inventory.previousGreenWeight', 'inventory.changeInGreenWeight', 'isPrivate',\n",
    "             'slug', 'updated_at', 'updatedAt', 'hardware']\n",
    "\n",
    "# Drop the extra data\n",
    "df.drop(other_meta, axis=1, inplace = True)\n",
    "display (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293029a-edef-4628-9af4-3819ee70f60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is where I may put a LOAD from the outside .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c15c47-8c29-4cd3-9ac1-0611fe1f0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Clean up and Enrich data  #\n",
    "#############################\n",
    "try:\n",
    "    # change dtypes\n",
    "    dtype_cols = ['weightGreen','weightRoasted','ambient', 'humidity', 'ambientTemp','roomHumidity']\n",
    "    df[dtype_cols] = df[dtype_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # combine RT2 ambientTemp and roomHumidity with RT3 ambient and humidity (note I am using ambient F)\n",
    "    ambientMeasurement = 'F'  #or C\n",
    "\n",
    "    df.loc[df['ambient'].isna(), 'ambient'] = df['ambientTemp'].astype(float)\n",
    "    df.loc[df['humidity'].isna(), 'humidity'] = df['roomHumidity'].astype(float)\n",
    "    df.drop(columns=['ambientTemp', 'roomHumidity','exitTemperature'], inplace =  True)  #dropping old environmental data and exitTemp\n",
    "\n",
    "    ###OPPORTUNITY to impute ambient temp where missing##\n",
    "    df.ambient.replace(to_replace = 0.0, value = np.nan, inplace = True) # some case of temp actually being 0.0 C or F will be excluded, but needed to remove outliers\n",
    "except KeyError:\n",
    "    print(\"Columns missing in repeated runs, skipping execution.\")\n",
    "\n",
    "    # convert dateTime\n",
    "    df['dateTime'] = pd.to_datetime(df['dateTime'], unit='ms')\n",
    "\n",
    "# Calculate -> weight lost percent = 100 * (green - roasted)/ green #   PLUS OTHERS\n",
    "df['weightLostPercent'] = 100 * (df['weightGreen'] - df['weightRoasted']) / df['weightGreen']\n",
    "df.loc[df['weightLostPercent'] > 17, 'weightLostPercent'] = np.nan  ## Future change to 50, 17 is too low\n",
    "\n",
    "# Fix low and high pre-heat errors (replace drumChargeTemperature w/ PH temp when z value > 3)\n",
    "# May want to edit raw json file if you still have bad preheat temps\n",
    "#df['drumChargeTemperature'].where(abs(stats.zscore(df.drumChargeTemperature-df.preheatTemperature)) < 3,\n",
    "#                                  df['preheatTemperature'], inplace = True)\n",
    "\n",
    "# calulate difference of beanDropTemp and beanChargeTemp (not ITBS, this should relative)   # maybe\n",
    "df['Drop-ChargeDeltaTemp'] = df['beanDropTemperature'] - df['beanChargeTemperature']\n",
    "\n",
    "# remove instances where FC was not picked or picked late \n",
    "## Would be better to impute FC values in the future\n",
    "df.loc[(df['indexFirstCrackStart'] > 2400) | (df['indexFirstCrackStart'] == 0), 'indexFirstCrackStart'] = np.nan\n",
    "df.loc[df['weightRoasted'] < 10, 'weightRoasted'] = np.nan\n",
    "df.loc[df['weightLostPercent'] > 50, 'weightLostPercent'] = np.nan\n",
    "    \n",
    "display (df.head(3))\n",
    "display (df.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be2bf1-8e73-4619-8b94-d37e46be55c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a133d-1660-4688-b5f2-02cf21fc0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# deconstruct temp curves from lists to new curve_df #\n",
    "######################################################\n",
    "## data for temp and derivative curve of each  is in a single cell as a list ###\n",
    "# For each roast (row) extract list of each curve data into a new DF and transpose,\n",
    "# add indexTime unique to each roast, add curve, concatenate to buddle all roasts curves.\n",
    "\n",
    "temp_curve_df = pd.DataFrame()\n",
    "curve_df = pd.DataFrame()\n",
    "for index, row in df.iterrows():\n",
    "        temp_curve_df = pd.DataFrame([pd.Series(row['beanTemperature'], name = 'beanTemperature', dtype='float64'), \n",
    "                                      pd.Series(row['drumTemperature'], name = 'drumTemperature', dtype='float64'),\n",
    "                                      pd.Series(row['beanDerivative'], name = 'beanDerivative', dtype='float64'),\n",
    "                                      pd.Series(row['ibtsDerivative'], name = 'ibtsDerivative', dtype='float64')]).T\n",
    "        temp_curve_df['indexTime'] = temp_curve_df.index\n",
    "        temp_curve_df['roastName'] = row['roastName']\n",
    "        temp_curve_df['softwareVersion'] = row['softwareVersion']\n",
    "        curve_df = pd.concat([curve_df, temp_curve_df],ignore_index = True) #update from #curve_df = curve_df.append(temp_curve_df,ignore_index = True) 1/23/24 for append depreciation\n",
    "\n",
    "# Calculate second derivative        \n",
    "# first pass at 2nd Derivative, review and see if it should be smoothed\n",
    "curve_df['ibts2ndDerivative'] = curve_df.groupby('roastName')['ibtsDerivative'].apply(lambda x:x.diff())\n",
    "\n",
    "#TO DO - Create 1stDerivative for roasts (.groupby('roastName')) with NaN itbsDerivative before Allio started adding it\n",
    "\n",
    "curve_df.fillna(value=np.nan, inplace=True)\n",
    "display (curve_df.head(3))\n",
    "display (curve_df.tail(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf54fc6-54fa-415b-8cf8-38c5d702bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "## Create df of point sets (single entry per profile) ##\n",
    "########################################################\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "point_list = ['beanChargeTemperature', 'beanDropTemperature', 'drumChargeTemperature',\n",
    "           'drumDropTemperature', 'preheatTemperature', 'roastStartIndex', 'roastEndIndex',\n",
    "           'totalRoastTime', 'indexFirstCrackStart', 'indexFirstCrackEnd', 'indexYellowingStart',\n",
    "           'weightGreen', 'weightRoasted', 'weightLostPercent','deltaTemp',\n",
    "           'roastNumber', 'sampleRate', 'firmware', 'missingSeconds',\n",
    "           'dateTime', 'roastName', 'comments', 'updatedAt',\n",
    "           'ambient', 'humidity', 'rating', 'beanId']\n",
    "point_df = pd.DataFrame(df, columns = point_list).reset_index()\n",
    "point_df.drop(columns='index', inplace = True)\n",
    "point_df.indexYellowingStart = point_df.indexYellowingStart.fillna(value=np.nan)\n",
    "point_df['totalRoastTime'] = point_df.totalRoastTime/60  # apparently totalRoastTime is counted in seconds not index steps\n",
    "display (point_df.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40914e1-1fee-4adb-b413-a5c4e8c70383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "### Enrich Point_DF ###\n",
    "#######################\n",
    "\n",
    "# Goals:\n",
    "# first(MAX ROR) [Done = TP, Yellow, FC, weight lost]\n",
    "# next (drying-malliard-dev times, and mean ROR between points & phases)\n",
    "# then (ROR and 2nd Derivitive average values  in each phase or between key points (185, 190))\n",
    "\n",
    "################################################################################################\n",
    "##  Find turning point index and index at 165 deg bean Temp (alt to inconsistently picked YP) ##\n",
    "################################################################################################\n",
    "roastName_df = curve_df.groupby(['roastName']) \n",
    "for name, group in roastName_df:\n",
    "    minBT = group.beanTemperature.min()  ### Get to 0 ROR indexTime (TP) via first occurance of minBT\n",
    "    for i,row in group.iterrows():\n",
    "        if row.beanTemperature == minBT and row.beanDerivative >= 0:   # multiple min points likely, so combined with first point climbing past 0 ROR\n",
    "           # print (name,i,row)  # Use this NEXT -  TO FIND WHAT 0 and nan problems are! 1/17\n",
    "            point_df.loc[(point_df.roastName == name),'indexTurningPoint'] = row.indexTime\n",
    "            point_df.loc[(point_df.roastName == name),'ibtsTurningPointTemp'] = row.drumTemperature\n",
    "            break\n",
    "    for i,row in group.iterrows():\n",
    "        if row.indexTime > 120 and row.drumTemperature >= 165:\n",
    "            autoYP165 = row.indexTime\n",
    "            point_df.loc[(point_df.roastName == name),'index165PT'] = autoYP165\n",
    "            break\n",
    "point_df['turningPointTime'] = (point_df.indexTurningPoint)/60/sampleRate\n",
    "\n",
    "\n",
    "# replace missing or bad YP pick with autoYP165   ### Probably should just switch all YP to autoYP165\n",
    "point_df.loc[(point_df.indexYellowingStart < 1), 'indexYellowingStart'] = point_df.index165PT\n",
    "point_df.loc[(point_df.indexYellowingStart.isnull()), 'indexYellowingStart'] = point_df.index165PT\n",
    "point_df['yellowPointTime'] = point_df.indexYellowingStart/60/sampleRate\n",
    "\n",
    "# replace bad FC points with np.nan. # Ryan why didn't you use the .replace() function?\n",
    "point_df.loc[(point_df.indexFirstCrackStart == 0),'indexFirstCrackStart'] = np.nan\n",
    "point_df.loc[(point_df.indexFirstCrackStart >10000),'indexFirstCrackStart'] = np.nan\n",
    "point_df['firstCrackTime'] = point_df.indexFirstCrackStart/60/2\n",
    "\n",
    "# time/temp\n",
    "point_df['time/temp'] = point_df.totalRoastTime/point_df.beanDropTemperature\n",
    "\n",
    "# ITBS BeanProbe difference for change over time plot\n",
    "point_df['deltaIBTS-BT'] = point_df.drumDropTemperature - point_df.beanDropTemperature\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10d55a-6af2-4352-a823-fe31214cd53b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###roast phases###\n",
    "## it is of the opinion that Drying, Browning, and Development are not ideal, \n",
    "## as the bean is still drying after yellowing, still browning after FC, and certainly developing before FC\n",
    "## thus Pre-YP, Pre-FC, Post-FC\n",
    "\n",
    "#point_df['pre-YellowPointPhase'] = \n",
    "#point_df['pre-FirstCrackPhase'] = \n",
    "#point_df['post-FirstCrackPhase'] = \n",
    "# Development Time Ratio (DTR)\n",
    "#point_df['DTR'] = \n",
    "\n",
    "#display(point_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7067f7-aaf8-4f5c-9f49-5a691041b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################     \n",
    "##  Save transformed data to .csv  ##\n",
    "#####################################\n",
    "\n",
    "#create subfolder if needed\n",
    "subfolder = 'csvExports/'\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "now = datetime.now()\n",
    "currentDateTime = now.strftime(\"%Y-%m-%d_%H%-M\")\n",
    "\n",
    "df.to_csv(subfolder + r'df_bulkData_' + currentDateTime + '.csv')   # may need   , index=False)\n",
    "curve_df.to_csv(subfolder + r'curve_df_' + currentDateTime + '.csv')\n",
    "point_df.to_csv(subfolder + r'point_df_' + currentDateTime + '.csv')\n",
    "\n",
    "display ('Data frames saved in ' + subfolder + 'folder with current Date_Time ' + currentDateTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877765e6-6285-473d-bcd0-e3b2f81c6c55",
   "metadata": {},
   "source": [
    "### ----------------\n",
    "### BREAK HERE: LOAD AND TRANSFORM ABOVE - VIEW AND ANALYZE BELOW\n",
    "### ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4610d63d-99b0-486f-9aaa-ba10a29330e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display (df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63bcb72-42ea-4b7a-a667-62ea76b14820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
